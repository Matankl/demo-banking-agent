{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d0f12511-9ecb-46b7-86ff-69c2d561899c",
   "metadata": {},
   "source": [
    "# Application deployment\n",
    "\n",
    "This notebook demonstrates the end-to-end process of building, testing, and deploying a banking virtual assistant using MLRun, LangChain, and Milvus for vector storage. It covers project setup, data ingestion for retrieval-augmented generation, application graph definition with guardrails and analysis steps, local testing, and deployment to Kubernetes. An interactive Gradio UI is also provided for user testing and demonstration.\n",
    "\n",
    "![](images/03_application_deployment_architecture.png)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "collapsed": false
   },
   "source": "# %pip install -r requirements.txt",
   "id": "ce216d94f07fc6b2",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "78f8c28c-0fe4-40a5-857d-6f7d9ebb0832",
   "metadata": {},
   "source": [
    "import mlrun\n",
    "from langchain_community.document_loaders import UnstructuredMarkdownLoader\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from dotenv import load_dotenv\n",
    "from langchain_huggingface.embeddings import HuggingFaceEmbeddings\n",
    "import warnings\n",
    "from langchain_milvus import Milvus\n",
    "\n",
    "load_dotenv(\"ai_gateway.env\")\n",
    "mlrun.set_env_from_file(\"ai_gateway.env\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "da9ee828-76d7-4ca0-8c77-dca5acc1647c",
   "metadata": {},
   "source": [
    "### Setup the project\n",
    "\n",
    "Load the project already created in the first notebook."
   ]
  },
  {
   "cell_type": "code",
   "id": "429394bc-7db7-45b2-80b8-ee3c60c11639",
   "metadata": {},
   "source": "project = mlrun.get_or_create_project(\"banking-agent\", user_project=True)",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "32cd6a39",
   "metadata": {},
   "source": [
    "This tutorial uses [Milvus](https://milvus.io/api-reference/pymilvus/v2.4.x/About.md) on a local host for simplicity. To use Milvus without the local host, see [Manage Milvus Connections](https://milvus.io/docs/v2.1.x/manage_connection.md).\n",
    "\n",
    "##### Note! since this tutorial uses Milvus on local the deployment will work only if you are running on the notebook service, consider running this on the notebook service in IGZ"
   ]
  },
  {
   "cell_type": "code",
   "id": "0586ca23-c750-40e3-846f-b17b7d9cb98f",
   "metadata": {},
   "source": [
    "import os\n",
    "\n",
    "db_path = os.path.join(os.getcwd(), \"milvus_demo.db\")\n",
    "if os.path.exists(db_path):\n",
    "    os.remove(db_path)\n",
    "\n",
    "MILVUS_ARGS = {\"uri\": db_path}\n",
    "MILVUS_ARGS"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "b347462d-9422-4b00-967b-ae68572f27b6",
   "metadata": {},
   "source": [
    "### Ingest data for vector store retrieval\n",
    "\n",
    "This section covers the ingestion of banking knowledge base documents into the Milvus vector store. Loading and embedding markdown files containing general bank information, account details, and customer FAQs, enables efficient retrieval-augmented generation for the virtual assistant. The following steps demonstrate how to load, embed, and store these documents for downstream use in the application."
   ]
  },
  {
   "cell_type": "code",
   "id": "de50a6a6-2de1-4fdc-a0a1-b1ce2216b523",
   "metadata": {},
   "source": [
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning, module=\"pkg_resources\")\n",
    "\n",
    "openai_available = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not openai_available:\n",
    "    embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
    "else:\n",
    "    embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "\n",
    "vectorstore = Milvus(\n",
    "    collection_name=\"banking_agent\",\n",
    "    embedding_function=embeddings,\n",
    "    connection_args=MILVUS_ARGS,\n",
    "    auto_id=True,\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "0b84cc40-0c3d-45a2-9b27-f72626c2172e",
   "metadata": {},
   "source": [
    "if not vectorstore.col:\n",
    "    general_bank_info_kb = UnstructuredMarkdownLoader(\n",
    "        \"data/general_bank_info_kb.md\"\n",
    "    ).load()\n",
    "    checking_savings_kb = UnstructuredMarkdownLoader(\n",
    "        \"data/checking_savings_kb.md\"\n",
    "    ).load()\n",
    "    customer_faq = UnstructuredMarkdownLoader(\"data/customer_faq.md\").load()\n",
    "    pages = general_bank_info_kb + checking_savings_kb + customer_faq\n",
    "    vectorstore.add_documents(pages)\n",
    "\n",
    "\n",
    "milvus_artifact = project.log_artifact(\n",
    "    \"vectorstore\", local_path=MILVUS_ARGS['uri'])\n",
    "\n",
    "MILVUS_ARGS['uri'] = milvus_artifact.uri"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "636f9863-4c3f-4eaf-ba24-c69008d75f4e",
   "metadata": {},
   "source": [
    "vectorstore.col.num_entities"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "941e20f5-6792-4b2c-9022-5923340f481f",
   "metadata": {},
   "source": [
    "### Define application serving graph\n",
    "\n",
    "The application serving graph orchestrates the flow of user queries through a series of modular steps:\n",
    "\n",
    "- **Input guardrails:**  \n",
    "    Ensure only safe and relevant queries proceed by filtering out inappropriate or off-topic inputs.\n",
    "\n",
    "- **Sentiment & churn analysis:**  \n",
    "    Analyze user sentiment and predict churn propensity to enrich the context for downstream processing.\n",
    "\n",
    "- **Context building:**  \n",
    "    Aggregate user information, sentiment, and churn data to construct a detailed context for the agent.\n",
    "\n",
    "- **Response generation:**  \n",
    "    The agent leverages the built context and retrieval-augmented generation from the vector store to provide accurate and personalized responses.\n",
    "\n",
    "This **graph-based architecture** enables robust, explainable, and extensible deployment of the banking virtual assistant, ensuring each step is modular and transparent for easier maintenance and future enhancements.\n",
    "\n",
    "See real-time serving graphs in the [documentation](https://docs.mlrun.org/en/stable/serving/serving-graph.html) for more information."
   ]
  },
  {
   "cell_type": "code",
   "id": "e9098c21-1f07-4fea-b346-2f4805d3834c",
   "metadata": {},
   "source": [
    "banking_topic_guardail = project.get_function(\"banking-topic-guardrail\")\n",
    "toxicity_guardrail = project.get_function(\"toxicity-guardrail\")\n",
    "churn_model = project.get_function(\"serving\")\n",
    "agent_graph = project.get_function(\"banking-agent\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "7f1bde28",
   "metadata": {},
   "source": [
    "mlrun.get_run_db().get_function(\"serving\", project=project.name)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43c3826e-ad04-4881-9fdc-339ce01102d6",
   "metadata": {},
   "source": [
    "from mlrun.serving import ModelRunnerStep\n",
    "\n",
    "graph = agent_graph.set_topology(\"flow\", engine=\"async\", exist_ok=True)\n",
    "# Step to process the input this step is there to make it invocation simpler with less arguments\n",
    "graph.add_step(\n",
    "    name=\"enrich_request\",\n",
    "    handler= \"enrich_request\",\n",
    ")\n",
    "\n",
    "# Topic and toxicity guardrail router (from notebook 2)\n",
    "guardrails_router = graph.add_step(\n",
    "    \"*ParallelRunMerger\",\n",
    "    name=\"input-guardrails\",\n",
    "    output_key=\"guardrails_output\",\n",
    "    extend_event=True,\n",
    "    after=\"enrich_request\"\n",
    ")\n",
    "guardrails_router.add_route(\n",
    "    key=\"banking-topic-guardrail\",\n",
    "    class_name=\"mlrun.serving.remote.RemoteStep\",\n",
    "    method=\"POST\",\n",
    "    url=banking_topic_guardail.get_url(),\n",
    ")\n",
    "guardrails_router.add_route(\n",
    "    key=\"toxicity-guardrail\",\n",
    "    class_name=\"mlrun.serving.remote.RemoteStep\",\n",
    "    method=\"POST\",\n",
    "    url=toxicity_guardrail.get_url(),\n",
    ")\n",
    "\n",
    "# Filtering accept and reject\n",
    "graph.add_step(\n",
    "    name=\"guardrail-filter\",\n",
    "    class_name=\"GuardrailsChoice\",\n",
    "    mapping={\"True\": \"accept\", \"False\": \"reject\"},\n",
    "    after=\"input-guardrails\",\n",
    ")\n",
    "\n",
    "graph.add_step(name=\"accept\", handler=\"accept\", after=\"guardrail-filter\")\n",
    "\n",
    "# Add model runner step to run the sentiment and churn analysis\n",
    "model_runner_step = ModelRunnerStep(\n",
    "    name=\"input-analysis\",\n",
    "    result_path=\"input_analysis_output\",\n",
    "    )\n",
    "model_runner_step.add_model(\n",
    "    model_class=\"SentimentAnalysisModelServer\",\n",
    "    endpoint_name=\"sentiment_analysis_output\",\n",
    "    result_path=\"sentiment_analysis_output\",\n",
    "    execution_mechanism=\"naive\",\n",
    ")\n",
    "model_runner_step.add_model(\n",
    "    model_class=\"ChurnModelServer\",\n",
    "    endpoint_name=\"churn_model_output\",\n",
    "    execution_mechanism=\"naive\",\n",
    "    dataset=f\"store://datasets/{project.name}/data-process-data_test#0:latest\",\n",
    "    label_column=\"churn\",\n",
    "    endpoint_url=churn_model.get_url(),\n",
    "    churn_mappings={\"high\": 0.50, \"medium\": 0.20, \"low\": 0},\n",
    "    result_path=\"churn_model_output\",)\n",
    "\n",
    "graph.add_step(model_runner_step, after=[\"accept\"], full_event= True,)\n",
    "\n",
    "\n",
    "graph.add_step(\n",
    "    name=\"build-context\",\n",
    "    class_name=\"BuildContext\",\n",
    "    context_mappings = {\n",
    "        \"name\": \"sentiment_analysis_output.name\",  # name is nested inside sentiment_analysis_output\n",
    "        \"sentiment\": \"sentiment_analysis_output.response[0]\",  # direct path, no input_analysis_output wrapper\n",
    "        \"churn\": \"churn_model_output.response[0]\",  # direct path\n",
    "    },\n",
    "    output_key=\"formatted_prompt\",\n",
    "    prompt=\"\"\"\n",
    "    This is context about the user and their query:\n",
    "    <user_context>\n",
    "    name: {name}\n",
    "    sentiment: {sentiment}\n",
    "    churn propensity percentage: {churn}\n",
    "    </user_context>\n",
    "\n",
    "    If they have a high churn propensity consider asking them if would like to escalate to a human operator.\n",
    "    Do not offer to escalate for low churn propensity.\n",
    "    Do NOT mention the churn propensity but use it to craft your response.\n",
    "    Use the sentiment to craft your response.\n",
    "    \"\"\",\n",
    "    after=\"input-analysis\",\n",
    "    full_event= True,\n",
    ")\n",
    "# Add the BankingAgent LLM using HF or OpenAI (if OPENAI credentials)\n",
    "MRS_banking_agent = ModelRunnerStep(name=\"banking-agent\")\n",
    "\n",
    "if not openai_available:\n",
    "    MRS_banking_agent.add_model(\n",
    "        model_class=\"BankingAgentHuggingFace\",\n",
    "        endpoint_name=\"BankingAgentHuggingFace\",\n",
    "        execution_mechanism=\"naive\",\n",
    "        model_name=os.environ.get(\"HF_MODEL_NAME\", \"Qwen/Qwen2.5-1.5B-Instruct\"),\n",
    "        prompt_input_key=\"formatted_prompt\",\n",
    "        messages_input_key=\"inputs\",\n",
    "        max_new_tokens=256,\n",
    "        temperature=0.2,\n",
    "        result_path=\"banking-agent\",\n",
    "    )\n",
    "else:\n",
    "    MRS_banking_agent.add_model(\n",
    "        model_class=\"BankingAgentOpenAI\",\n",
    "        endpoint_name=\"BankingAgentOpenAI\",\n",
    "        execution_mechanism=\"naive\",\n",
    "        model_name=\"gpt-4o-mini\",\n",
    "        system_prompt=\"You are a helpful assistant for IGZ Bank. Respond in a concise, but detailed way. Use web search if the customer asks about other banks or external information.\",\n",
    "        result_path=\"banking-agent\",\n",
    "        after=\"build-context\",\n",
    "        prompt_input_key=\"formatted_prompt\",\n",
    "        messages_input_key=\"inputs\",\n",
    "        vector_db_collection=\"banking_agent\",\n",
    "        vector_db_args=MILVUS_ARGS,\n",
    "        vector_db_description=\"Use this to answer any questions about general bank info like locations, hours, guidelines for opening savings/checking accounts, APY for savings/checking, as well as general FAQ like resetting passwords, ATM fees, setting up direct deposit, etc.\",\n",
    "    )\n",
    "\n",
    "graph.add_step(MRS_banking_agent, after=[\"build-context\"])\n",
    "\n",
    "graph.add_step(name=\"reject\", handler=\"reject\", after=\"guardrail-filter\")\n",
    "\n",
    "graph.add_step(\n",
    "    name=\"output\", handler=\"responder\", after=[\"banking-agent\", \"reject\"]\n",
    ").respond()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8fc9c2cb-0569-4bca-a652-ede274c18a83",
   "metadata": {},
   "source": "graph.plot(rankdir=\"LR\")",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Since running the LLM model is very resource demanding some systems can't run it locally so we will use the mock server only with OpenAI",
   "id": "294bec722b745a15"
  },
  {
   "cell_type": "code",
   "id": "a7b33baa-d143-49a9-8007-6bade49b6813",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    mock = agent_graph.to_mock_server()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "aff51457-0679-415e-8c95-c83fcb0ce773",
   "metadata": {},
   "source": [
    "### Test the input guardrails"
   ]
  },
  {
   "cell_type": "code",
   "id": "dd31b609-e5bb-489c-a102-a7ddeca9cf52",
   "metadata": {},
   "source": [
    "HIGH_PROPENSITY_CHURN_USER_ID = 32\n",
    "LOW_PROPENSITY_CHURN_USER_ID = 2296\n",
    "\n",
    "def _format_question(question: str, role: str = \"user\"):\n",
    "    return {\"role\": role, \"content\": question}"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "af96d665",
   "metadata": {},
   "source": [
    "Question the agent CANNOT answer - rejects input"
   ]
  },
  {
   "cell_type": "code",
   "id": "a8f936c3-2776-4b4c-b18e-184a94fc4c8d",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp = mock.test(\n",
    "        path=\"/\",\n",
    "        body={\n",
    "            \"name\": \"John\",\n",
    "            \"inputs\": [_format_question(\"What is a mortgage, from the bank?\")],\n",
    "            \"user_id\": LOW_PROPENSITY_CHURN_USER_ID,\n",
    "        },\n",
    "    )\n",
    "    print(resp[\"outputs\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f7c17455",
   "metadata": {},
   "source": [
    "Question the agent CANNOT answer - rejects input"
   ]
  },
  {
   "cell_type": "code",
   "id": "6406eeea-1849-4aab-b6f0-a7e1d1f18138",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp = mock.test(\n",
    "        path=\"/\",\n",
    "        body={\n",
    "            \"name\": \"John\",\n",
    "            \"inputs\": [_format_question(\"i hate you\")],\n",
    "            \"user_id\": LOW_PROPENSITY_CHURN_USER_ID,\n",
    "        },\n",
    "    )\n",
    "    print(resp[\"outputs\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "d7c61393-228f-4a61-bd4c-cbbba2a44f71",
   "metadata": {},
   "source": [
    "### Test the banking agent - sentiment analysis and churn propensity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e7fe1b",
   "metadata": {},
   "source": [
    "Standard Q&A with neutral sentiment and low churn"
   ]
  },
  {
   "cell_type": "code",
   "id": "93faf1a7-48d4-46c5-8571-03b90d2ce7b9",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp = mock.test(\n",
    "        path=\"/\",\n",
    "        body={\n",
    "            \"name\": \"John\",\n",
    "            \"inputs\": [_format_question(\"how to apply for checking account?\")],\n",
    "            \"user_id\": LOW_PROPENSITY_CHURN_USER_ID,\n",
    "        },\n",
    "    )\n",
    "    print(resp[\"outputs\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "dc862c77",
   "metadata": {},
   "source": [
    "Standard Q&A with negative sentiment and low churn"
   ]
  },
  {
   "cell_type": "code",
   "id": "91d0ebe4-9c8c-4463-857e-1cd61ea42a1e",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp = mock.test(\n",
    "        path=\"/\",\n",
    "        body={\n",
    "            \"name\": \"John\",\n",
    "            \"inputs\": [\n",
    "                _format_question(\n",
    "                    \"how to apply for checking account? I keep trying but I'm really frustrated\"\n",
    "                )\n",
    "            ],\n",
    "            \"user_id\": LOW_PROPENSITY_CHURN_USER_ID,\n",
    "        },\n",
    "    )\n",
    "    print(resp[\"outputs\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "923d9cc2",
   "metadata": {},
   "source": [
    "Standard Q&A with low sentiment and high churn - note that the model offers to escalate to a human operator. This kind of behavior is customizable depending on the input guardrails and input analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e1ad9d",
   "metadata": {},
   "source": [
    "Standard multi-turn Q&A"
   ]
  },
  {
   "cell_type": "code",
   "id": "79d62a7c-02d7-41cc-8b23-a8a418284db3",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp = mock.test(\n",
    "        path=\"/\",\n",
    "        body={\n",
    "            \"name\": \"Alice\",\n",
    "            \"inputs\": [\n",
    "                {\"role\": \"user\", \"content\": \"Hi—how do I open a checking account?\"},\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"To open a checking account, you need two forms of ID and a minimum deposit of $25.\",\n",
    "                },\n",
    "                {\"role\": \"user\", \"content\": \"Is it possible to get cashback rewards?\"},\n",
    "            ],\n",
    "            \"user_id\": HIGH_PROPENSITY_CHURN_USER_ID,  # <-- High churn propensity user\n",
    "        },\n",
    "    )\n",
    "    print(resp[\"outputs\"][0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8558e729-68a3-4e57-bbad-104e7943ea88",
   "metadata": {},
   "source": [
    "### Full outputs\n",
    "\n",
    "Below is the comprehensive output from the application graph. This includes all intermediate and final results: user input, guardrails decisions, input analysis (such as sentiment and churn predictions), any tool calls, and the generated response from the model. Use this section to trace the end-to-end flow and understand how each component contributes to the final answer."
   ]
  },
  {
   "cell_type": "code",
   "id": "24825160-fb6a-4852-b357-accd6106c033",
   "metadata": {},
   "source": [
    "if openai_available:\n",
    "    resp"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c763a375-d807-47cd-9c4e-0d43be3d0616",
   "metadata": {},
   "source": [
    "### Deploy to Kubernetes\n",
    "\n",
    "Deploy the banking agent application to a production-ready Kubernetes endpoint. This enables robust, scalable, and highly available access for integration with real-world applications. Kubernetes orchestration ensures automatic scaling and reliability based on demand."
   ]
  },
  {
   "cell_type": "code",
   "id": "b9d8f5ea-7536-445c-b208-5af8055f90ae",
   "metadata": {},
   "source": [
    "# Reminder! since milvus db is local the deployment will work only if you are running on the notebook service\n",
    "project.deploy_function(agent_graph)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "8d2ad4d5-8311-4507-a06a-907ef52151c6",
   "metadata": {},
   "source": [
    "resp = agent_graph.invoke(\n",
    "    path=\"/\",\n",
    "    body={\n",
    "        \"name\": \"Alice\",\n",
    "        \"inputs\": [{\"role\": \"user\", \"content\": \"Hi, how do I open a checking account?\"}],\n",
    "        \"user_id\": HIGH_PROPENSITY_CHURN_USER_ID,  # <-- High churn propensity user\n",
    "    },\n",
    ")\n",
    "print(resp)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "resp = agent_graph.invoke(\n",
    "    path=\"/\",\n",
    "    body={\n",
    "        \"name\": \"Alice\",\n",
    "        \"inputs\": [{\"role\": \"user\", \"content\": \"what is a mortgage?\"}],\n",
    "        \"user_id\": HIGH_PROPENSITY_CHURN_USER_ID,  # <-- High churn propensity user\n",
    "    },\n",
    ")\n",
    "print(resp)"
   ],
   "id": "b77bcfda7be45b56"
  },
  {
   "cell_type": "markdown",
   "id": "53049435-bde9-4d5c-9313-f2af716cb1ee",
   "metadata": {},
   "source": [
    "### Application UI\n",
    "\n",
    "<div style=\"padding: 10px; border-left: 6px solid #f0ad4e; background: #fcf8e3;\">\n",
    "<b>⚠️ Warning:</b> This section is not supported on Community Edition.\n",
    "</div><br>\n",
    "\n",
    "The Streamlit UI offers an interactive environment to test and explore the banking agent's capabilities. Its main features include:\n",
    "\n",
    "- **Chat window:**  \n",
    "    Engage in a conversational interface where you can enter questions and receive responses from the assistant, simulating real user interactions.\n",
    "\n",
    "- **Tool usage visualization:**  \n",
    "    When the agent invokes external tools or APIs (such as retrieving information from the vector store), these actions are surfaced in the chat, allowing you to see when and how tools are used.\n",
    "\n",
    "- **Intermediate graph steps:**  \n",
    "    The UI displays outputs from key stages of the application graph, including:\n",
    "    - **Input guardrails:**  \n",
    "        - *Toxicity guardrail passed* — Indicates if the input passes the toxicity filter.\n",
    "        - *Banking topic guardrail passed* — Shows whether the query is relevant to banking topics.\n",
    "    - **Input analysis:**  \n",
    "        - *Sentiment analysis* — Reveals the detected sentiment of the user's input.\n",
    "        - *Churn prediction* — Estimates the user's propensity to leave, which can influence the assistant's response.\n",
    "\n",
    "This transparent design helps users trace the end-to-end flow of their queries, understand decision points, and gain insight into how each system component contributes to the final answer.\n",
    "\n",
    "![](images/banking_agent_ui.png)"
   ]
  },
  {
   "cell_type": "code",
   "id": "f4aaed25-99ae-4bf2-900f-d324b789940c",
   "metadata": {},
   "source": [
    "!tar -czvf frontend_ui.tar.gz ./src/functions/frontend_ui.py"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "bd98e5d9-7df4-41a1-bf2b-e3c294066217",
   "metadata": {},
   "source": [
    "frontend_source = project.log_artifact(\"frontend_source\", local_path=\"frontend_ui.tar.gz\", upload=True)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "9497a840-0ee1-40c7-89aa-28ed79162ca1",
   "metadata": {},
   "source": [
    "ui_fn = project.set_function(\n",
    "    name=\"frontend\",\n",
    "    kind=\"application\",\n",
    "    image=\"mlrun/mlrun\",\n",
    "    requirements=[\"streamlit==1.49.1\"]\n",
    ")\n",
    "\n",
    "API_URL = agent_graph.get_url()\n",
    "API_URL"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "ddbffddc-5b14-4415-99fd-fc1daf21c766",
   "metadata": {},
   "source": [
    "ui_fn.set_env(\"API_URL\", API_URL)\n",
    "ui_fn.with_source_archive(frontend_source.target_path, pull_at_runtime=False)\n",
    "ui_fn.set_internal_application_port(8000)\n",
    "ui_fn.spec.command = \"streamlit\"\n",
    "ui_fn.spec.args = [\"run\", \"--server.port\", \"8000\", \"/home/mlrun_code/src/functions/frontend_ui.py\"]"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "1cce035c-8c22-4207-a771-992867c96c09",
   "metadata": {},
   "source": [
    "ui_fn.deploy(with_mlrun=False, create_default_api_gateway=False)\n",
    "ui_fn.create_api_gateway(\n",
    "    name=\"banking-agent-ui\",\n",
    "    path=\"/\",\n",
    "    direct_port_access=True,\n",
    "    ssl_redirect=True,\n",
    "    set_as_default=False,\n",
    "    authentication_mode=\"none\"\n",
    ")"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
